# ğŸš€ MCP-Ollama Server

<div align="center">


*Connect the power of Model Context Protocol with local LLMs*

[![GitHub license](https://img.shields.io/github/license/sethuram2003/mcp-ollama_server)](https://github.com/sethuram2003/mcp-ollama_server/blob/main/LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/sethuram2003/mcp-ollama_server?style=social)](https://github.com/sethuram2003/mcp-ollama_server/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/sethuram2003/mcp-ollama_server?style=social)](https://github.com/sethuram2003/mcp-ollama_server/network/members)
[![GitHub issues](https://img.shields.io/github/issues/sethuram2003/mcp-ollama_server)](https://github.com/sethuram2003/mcp-ollama_server/issues)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)

[Getting Started](#-quick-start) â€¢ 
[Features](#-key-features) â€¢ 
[Architecture](#-architecture) â€¢ 
[Documentation](#-documentation) â€¢ 
[Contributing](#-contributing) â€¢ 
[FAQ](#-faq)

</div>

## ğŸ“‹ Overview

**MCP-Ollama Server** bridges the gap between Anthropic's Model Context Protocol (MCP) and local LLMs via Ollama. This integration empowers your on-premise AI models with Claude-like tool capabilities, including file system access, calendar integration, web browsing, email communication, GitHub interactions, and AI image generationâ€”all while maintaining complete data privacy.

Unlike cloud-based AI solutions, MCP-Ollama Server:
- Keeps all data processing on your local infrastructure
- Eliminates the need to share sensitive information with third parties
- Provides a modular approach that allows you to use only the components you need
- Enables enterprise-grade AI capabilities in air-gapped or high-security environments



## âœ¨ Key Features

- **ğŸ”’ Complete Data Privacy**: All computations happen locally through Ollama
- **ğŸ”§ Tool Use for Local LLMs**: Extends Ollama models with file, calendar, and other capabilities
- **ğŸ§© Modular Architecture**: Independent Python service modules that can be deployed selectively
- **ğŸ”Œ Easy Integration**: Simple APIs to connect with existing applications
- **ğŸš€ Performance Optimized**: Minimal overhead to maintain responsive AI interactions
- **ğŸ“¦ Containerized Deployment**: Docker support for each module (coming soon)
- **ğŸ§ª Extensive Testing**: Comprehensive test coverage for reliability

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+ installed
- [Ollama](https://ollama.ai/) set up on your system
- Git for cloning the repository


## ğŸ§© Component Overview

MCP-Ollama Server is organized into specialized modules, each providing specific functionality:

### ğŸ“… Calendar Module

```
calendar/
â”œâ”€â”€ README.md          # Module-specific documentation
â”œâ”€â”€ calendar_mcp.py    # MCP implementation for calendar operations
â”œâ”€â”€ google_calendar.py # Google Calendar API integration
â”œâ”€â”€ pyproject.toml     # Dependencies and package info
â”œâ”€â”€ test.py            # Test cases for calendar functionality
â””â”€â”€ time_test.py       # Time-related utility tests
```

The Calendar module enables your local LLM to:
- Create, modify, and delete calendar events
- Check availability and scheduling conflicts
- Send meeting invitations
- Set reminders and notifications

### ğŸ”„ Client MCP Module

```
client_mcp/
â”œâ”€â”€ README.md      # Module-specific documentation
â”œâ”€â”€ client.py      # Main client implementation
â”œâ”€â”€ pyproject.toml # Dependencies and package info
â”œâ”€â”€ testing.txt    # Test data
â””â”€â”€ uv.lock        # Dependency lock file
```

The Client module provides:
- A unified interface to interact with all MCP-enabled services
- Conversation history management
- Context handling for improved responses
- Tool selection and routing logic

### ğŸ“ File System Module

```
file_system/
â”œâ”€â”€ README.md          # Module-specific documentation
â”œâ”€â”€ file_system.py     # File system operations implementation
â”œâ”€â”€ pyproject.toml     # Dependencies and package info
â””â”€â”€ uv.lock            # Dependency lock file
```

The File System module allows your local LLM to:
- Read and write files securely
- List directory contents
- Search for files matching specific patterns
- Parse different file formats (text, CSV, JSON, etc.)

### Installation

```bash
# 1. First install uv if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. Clone the repository
git clone https://github.com/sethuram2003/mcp-ollama_server.git
cd mcp-ollama_server

# 3. Verify Ollama model is installed (replace 'llama3' with your preferred model)
ollama pull llama3
```

### Module Configuration

1. **ğŸ“… Calendar Module:**

```bash
cd calendar
uv add pyproject.toml  # Install calendar-specific dependencies
```

2. **ğŸ”„ Client MCP Module:**

```bash
cd client_mcp
uv add pyproject.toml  # Install calendar-specific dependencies
```

3. **ğŸ“ File System Module:**

```python
cd file_system
uv add pyproject.toml  # Install filesystem dependencies
```
### Usage

```bash
cd client_mcp
uv run client.py ../file_system/file_system.py
```

## ğŸ—ï¸ Architecture

MCP-Ollama Server follows a microservices architecture pattern, where each capability is implemented as an independent service:



### Key Components:

1. **Ollama Integration Layer**: Connects to your local Ollama instance and routes appropriate requests
2. **MCP Protocol Handlers**: Translate between standard MCP format and Ollama's requirements
3. **Service Modules**: Independent modules that implement specific capabilities
4. **Client Library**: Provides a unified interface for applications to interact with the system

This architecture provides several benefits:
- **Scalability**: Add new modules without affecting existing ones
- **Resilience**: System continues functioning even if individual modules fail
- **Flexibility**: Deploy only the components you need
- **Security**: Granular control over data access for each module

## ğŸ“š Documentation

### Module-Specific Documentation

Each module contains its own README with detailed implementation notes:

- [Calendar Module Documentation](calendar/README.md)
- [Client MCP Documentation](client_mcp/README.md)
- [File System Documentation](file_system/README.md)

## ğŸ› ï¸ Use Cases

### Enterprise Security & Compliance

Ideal for organizations that need AI capabilities but face strict data sovereignty requirements:
- Legal firms processing confidential case files
- Healthcare providers analyzing patient data
- Financial institutions handling sensitive transactions

### Developer Productivity

Transform your local development environment:
- Code generation with access to your project files
- Automated documentation based on codebase analysis
- Integration with local git repositories

### Personal Knowledge Management

Create a powerful second brain that respects your privacy:
- Process personal documents and notes
- Manage calendar and schedule optimization
- Generate content based on your personal knowledge base

## ğŸ¤ Contributing

We welcome contributions from the community! Here's how you can help:

1. **Fork the Repository**: Create your own fork of the project
2. **Create a Feature Branch**: `git checkout -b feature/amazing-feature`
3. **Make Your Changes**: Implement your feature or bug fix
4. **Run Tests**: Ensure your changes pass all tests
5. **Commit Changes**: `git commit -m 'Add some amazing feature'`
6. **Push to Branch**: `git push origin feature/amazing-feature`
7. **Open a Pull Request**: Submit your changes for review

Please read our [Contributing Guidelines](CONTRIBUTING.md) for more details.

## â“ FAQ

**Q: How does this differ from using cloud-based AI assistants?**  
A: MCP-Ollama Server runs entirely on your local infrastructure, ensuring complete data privacy and eliminating dependence on external APIs.

**Q: What models are supported?**  
A: Any model compatible with Ollama can be used. For best results, we recommend Llama 3, Mistral, or other recent open models with at least 7B parameters.

**Q: How can I extend the system with new capabilities?**  
A: Follow the modular architecture pattern to create new service modules. See our [Extension Guide](docs/extension.md) for details.

**Q: What are the system requirements?**  
A: Requirements depend on the Ollama model you choose. For basic functionality, we recommend at least 16GB RAM and a modern multi-core CPU.

## ğŸ“„ License

This project is licensed under the terms included in the [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgements

- [Anthropic](https://www.anthropic.com/) for the Model Context Protocol specification
- [Ollama](https://ollama.ai/) for their excellent local LLM server

---

<div align="center">
  <p><strong>MCP-Ollama Server</strong> - Bringing cloud-level AI capabilities to your local environment</p>
  <p>
    <a href="https://github.com/sethuram2003/mcp-ollama_server/stargazers">â­ Star us on GitHub</a> â€¢
    <a href="https://github.com/sethuram2003/mcp-ollama_server/issues">ğŸ› Report Bug</a> â€¢
    <a href="https://github.com/sethuram2003/mcp-ollama_server/issues">âœ¨ Request Feature</a>
  </p>
</div>
